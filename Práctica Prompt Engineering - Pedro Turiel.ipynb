{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FyBFdYnPnoBp",
        "JosfnQ2PBJQ8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ejercicio 1: Crea un prompt temático"
      ],
      "metadata": {
        "id": "FyBFdYnPnoBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El enunciado de este primer ejercicio es el siguiente: En este ejercicio, deberás diseñar un prompt utilizando las técnicas explicadas en clase, basado en una temática específica de tu elección. El objetivo es que el prompt sea claro, coherente y detallado, permitiendo obtener una respuesta precisa y\n",
        "relevante.\n",
        "\n",
        "Como puede apreciarse, lo que se solicita en este ejercicio es generar un prompt automático sobre una temática a elegir y que en base este prompt, se obtenga una respuesta acorde a lo preguntado. Para ello, lo primero que hay que hacer es decidir la temática. Dado que se debe solicitar al usuario una serie de campos y en base a esa información se tiene que generar el prompt, la temática a elegir debe ser una que permita solicitar varios apartados al usuario.\n",
        "\n",
        "Tras haberlo pensado he decidido que la temática del prompt va a ser la cocina y concretamente la generación de recetas ya que entre recetas hay muchos campos que se repiten (tiempo de ejecución, nivel de dificultad, ingredientes...) lo caul pueden ser los campos que se le soliciten al usuario. Es por eso que debido a su gran versatilidad, decido decantarme por esta temática.\n",
        "\n",
        "El modo en que voy a enfocar el ejercicio es el siguiente. En primer lugar, se solicitará al usuario que complete los campos específicos para la receta. Con la información de estos campos generaré el prompt y este prompt lo introduciré por medio de la API de OpenAI como un prompt de usuario. Finalmente, capturaré el mensaje obtenido y se imprimirá por pantalla la receta.\n",
        "\n",
        "Por ello, dado que se va utilizar openai y su API, el primer paso es instalar la librería."
      ],
      "metadata": {
        "id": "0OJdiIGynvBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgvTn9_7khOE",
        "outputId": "50fef868-a2ff-49fc-8c59-f6a4f176db85"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.30.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, voy a generar una función que es la que se va a encargar de generar el prompt. Voy a trabajar con funciones para que el ejercicio este más organizado y más limpio. Esta primera función se va a denominar generar_prompt y en ella se va a solicitar al usuario que complete una serie de campos y como respuesta se va a generar el prompt. Los campos que el usuario va a tener que completar son:\n",
        "\n",
        "* Tipo de cocina: Como puede ser una receta genérica, el usuario va a tener que especificar el tipo de cocina que desea cocinar. Por ejemplo, mexicana, italiana, japonesa...\n",
        "* Tiempo de preparación: El tiempo que el usuario desea emplear cocinando.\n",
        "* Número de porciones: Es necesario conocer el número de comensales para poder ajustar la receta.\n",
        "* Nivel de dificultad: No es lo mismo cocinar sushi que cocinar gyozas. Aunque ambas son cocina japonesa, el grado de dificultad no es el mismo.\n",
        "* Ingrediente principal: En la mayoría de recetas se suele indicar cual es el ingrediente principal, por lo que dejo la puerta abierta a esta posibilidad.\n",
        "* Equipamiento disponible: Puede ser interesante ofrecer la posibilidad de especificar el equipamiento del que se dispone para cocinar.\n",
        "* Preferencia dietética: Hoy en día existe mucha conciencia con la preferencia dietética por lo que me parece importante incluir este campo en la receta. Con preferencia dietética me refiero a opciones como vegano, vegetariano, sin gluten...\n",
        "\n",
        "Los campos que he definido no son igual de importantes desde mi punto de vista dado que para generar la recet no es necesario conocer la información de todos ellos. Por ejemplo, el campo de equipamiento disponible no es igual de importante que el campo tipo de cocina. Es por eso que los últimos tres campos van a ser opcionales para que el usuario decida si quiere completarlos o no. En caso de que se complete, al prompt se le añadirá la información adicional.  "
      ],
      "metadata": {
        "id": "lZxp-cqYp9pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "def generar_prompt():\n",
        "    #CAMPOS REQUERIDOS\n",
        "    tipo_cocina = input(\"Ingrese el tipo de cocina que desea cocinar: \")\n",
        "    tiempo_preparacion = input(\"Ingrese el tiempo de preparación (en minutos): \")\n",
        "    num_porciones = input(\"Ingrese el número de porciones: \")\n",
        "    nivel_dificultad = input(\"Ingrese el nivel de dificultad: fácil, intermedio o avanzado): \")\n",
        "\n",
        "    #CAMPOS ADICIONALES\n",
        "    ingredientes_principales = input(\"¿Cuáles son los ingredientes principales que desea usar? [Opcional]: \")\n",
        "    equipamiento_disponible = input(\"Especifique si tiene algún equipamiento especial disponible como pueder ser horno, sartén, olla de presión. [Opcional]: \")\n",
        "    preferencias_dieteticas = input(\"Indique cualquier preferencia dietética que tenga (e.g., sin gluten, bajo en carbohidratos, vegano) [Opcional]: \")\n",
        "\n",
        "    #GENERACIÓN DEL PROMPT\n",
        "    prompt = (f\"Basado en las siguientes especificaciones, escribe una receta de cocina {tipo_cocina} \"\n",
        "              f\"que se pueda preparar en {tiempo_preparacion} minutos para {num_porciones} personas. \"\n",
        "              f\"La receta debe ser de nivel {nivel_dificultad}.\")\n",
        "\n",
        "    #CAMPOS ADICIONALES\n",
        "    if ingredientes_principales:\n",
        "        prompt += f\" Debe incluir {ingredientes_principales} como ingredientes principales.\"\n",
        "    if equipamiento_disponible:\n",
        "        prompt += f\" Se dispone de {equipamiento_disponible}.\"\n",
        "    if preferencias_dieteticas:\n",
        "        prompt += f\" La receta debe ser {preferencias_dieteticas}.\"\n",
        "\n",
        "    #AÑADO ESTE ÚLTIMO PROMPT SIGUIENDO LA TÉNCICA DE IN-CONTEXT-INSTRUCTION\n",
        "    prompt += \" Por favor, incluye una lista de ingredientes completa, instrucciones paso a paso y cualquier consejo adicional para mejorar el plato.\"\n",
        "\n",
        "    return prompt\n",
        "\n"
      ],
      "metadata": {
        "id": "NyFBezVllXcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementada la función para generar el prompt, desarrollo la función para obtener la receta. En este caso, voy a utilizar la api_key proporcionada durante las clases y el modelo gpt-3.5-turbo. Como mensaje voy a generar dos prompts:uno a nivel de sistema que va a utilizar la técnica de RolePLay para que especificarle que se comporte como un chef experto  y otro que va a ser el propio prompt obtenido en la función anterior. Este segundo prompt se va a introducir a nivel de usuario. Por último la función generará como respuesta, el mensaje generado por OpenAI."
      ],
      "metadata": {
        "id": "-6b8YZ_kuV_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def obtener_receta(prompt):\n",
        "  client = OpenAI (api_key='sk-proj-jRwv8xxafg5nXJ00E13YT3BlbkFJtnk6RJhN4dpoh3sikN6H')\n",
        "  response = client.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Eres un chef experto.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "    temperature = 0.5)\n",
        "\n",
        "  return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "qtRnnSt-tlGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por último, llamo a las funciones y genero la receta."
      ],
      "metadata": {
        "id": "3VOSTFKLxqXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    prompt = generar_prompt()\n",
        "    receta = obtener_receta(prompt)\n",
        "    print(\"\\nReceta generada:\\n\")\n",
        "    print(receta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Xs7QsFaxnzS",
        "outputId": "7b00aa1a-81a0-4ef6-dbf0-f9649d89be8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingrese el tipo de cocina que desea cocinar: Italiana\n",
            "Ingrese el tiempo de preparación (en minutos): 60\n",
            "Ingrese el número de porciones: 6\n",
            "Ingrese el nivel de dificultad: fácil, intermedio o avanzado): Avanzado\n",
            "¿Cuáles son los ingredientes principales que desea usar? [Opcional]: Arroz\n",
            "Especifique si tiene algún equipamiento especial disponible como pueder ser horno, sartén, olla de presión. [Opcional]: Horno de leña\n",
            "Indique cualquier preferencia dietética que tenga (e.g., sin gluten, bajo en carbohidratos, vegano) [Opcional]: \n",
            "\n",
            "Receta generada:\n",
            "\n",
            "Receta de Risotto al horno con champiñones y parmesano\n",
            "\n",
            "Ingredientes:\n",
            "- 2 tazas de arroz Arborio\n",
            "- 1 litro de caldo de pollo\n",
            "- 1 cebolla grande, picada finamente\n",
            "- 3 dientes de ajo, picados\n",
            "- 500g de champiñones, en rodajas\n",
            "- 1 taza de vino blanco seco\n",
            "- 1 taza de queso parmesano rallado\n",
            "- 1/2 taza de mantequilla\n",
            "- Aceite de oliva\n",
            "- Sal y pimienta al gusto\n",
            "- Perejil fresco picado para decorar\n",
            "\n",
            "Instrucciones:\n",
            "\n",
            "1. Precalienta el horno de leña a 180°C (350°F).\n",
            "\n",
            "2. En una sartén grande a fuego medio, calienta un poco de aceite de oliva y derrite la mitad de la mantequilla. Agrega la cebolla y el ajo, y cocina hasta que estén transparentes.\n",
            "\n",
            "3. Añade los champiñones y cocina hasta que estén dorados y hayan soltado su líquido. Retira del fuego y reserva.\n",
            "\n",
            "4. En una fuente apta para horno, coloca el arroz Arborio y distribuye uniformemente. Agrega la mezcla de champiñones por encima.\n",
            "\n",
            "5. Vierte el vino blanco sobre el arroz y champiñones. Luego, agrega el caldo de pollo caliente hasta cubrir completamente los ingredientes.\n",
            "\n",
            "6. Cubre la fuente con papel de aluminio y lleva al horno de leña por 30 minutos.\n",
            "\n",
            "7. Pasado ese tiempo, retira el papel de aluminio y agrega el queso parmesano rallado y la mantequilla restante en trozos pequeños por encima del arroz.\n",
            "\n",
            "8. Hornea por otros 15-20 minutos, o hasta que el arroz esté cocido y el queso se haya derretido y dorado.\n",
            "\n",
            "9. Retira del horno y deja reposar por unos minutos antes de servir. Espolvorea perejil fresco picado por encima como decoración.\n",
            "\n",
            "Consejos adicionales:\n",
            "- Puedes agregar un toque de trufa rallada o aceite de trufa al final para darle un sabor más sofisticado al risotto.\n",
            "- Asegúrate de ir agregando caldo caliente al arroz durante la cocción para mantener la textura cremosa característica del risotto.\n",
            "- Acompaña este plato con una ensalada fresca y un buen vino blanco para una experiencia culinaria completa. ¡Disfruta de tu Risotto al horno con champiñones y parmesano!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El primer ejemplo que he utilizado es el de la cocina italiana y como puede apreciarse se ha obtenido una receta para preprara risotto en base a los argumentos introducidos en el prompt. Es posible ver como la receta obtenida se adecua a estos parámetros ya que por ejemplo está teniendo en cuenta el horno de leña, el arroz, el tipo de cocina italiana...\n",
        "\n",
        "Voy a generar otro ejemplo para ver si funciona correctamente con otros parámetros."
      ],
      "metadata": {
        "id": "gZjbb4TYygkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    prompt = generar_prompt()\n",
        "    receta = obtener_receta(prompt)\n",
        "    print(\"\\nReceta generada:\\n\")\n",
        "    print(receta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVCUADwkx-OA",
        "outputId": "4fe0eded-9317-4647-e390-c0e3c006c4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingrese el tipo de cocina que desea cocinar: Española\n",
            "Ingrese el tiempo de preparación (en minutos): 120\n",
            "Ingrese el número de porciones: 4\n",
            "Ingrese el nivel de dificultad: fácil, intermedio o avanzado): Intermedio\n",
            "¿Cuáles son los ingredientes principales que desea usar? [Opcional]: Bacalao\n",
            "Especifique si tiene algún equipamiento especial disponible como pueder ser horno, sartén, olla de presión. [Opcional]: \n",
            "Indique cualquier preferencia dietética que tenga (e.g., sin gluten, bajo en carbohidratos, vegano) [Opcional]: Sin gluten\n",
            "\n",
            "Receta generada:\n",
            "\n",
            "Receta de Bacalao al Pil Pil\n",
            "\n",
            "Ingredientes:\n",
            "- 800g de lomos de bacalao desalado\n",
            "- 1 taza de aceite de oliva virgen extra\n",
            "- 4 dientes de ajo, en rodajas finas\n",
            "- 1 guindilla roja, sin semillas y picada\n",
            "- Sal y pimienta al gusto\n",
            "- Perejil fresco picado para decorar\n",
            "\n",
            "Instrucciones:\n",
            "\n",
            "1. Seca bien los lomos de bacalao con papel de cocina y córtalos en trozos medianos. Reserva.\n",
            "\n",
            "2. En una sartén grande a fuego medio, calienta el aceite de oliva. Añade los ajos en rodajas y la guindilla picada. Cocina suavemente hasta que los ajos estén dorados pero no quemados, aproximadamente 5 minutos.\n",
            "\n",
            "3. Retira la sartén del fuego y deja que el aceite se enfríe un poco. Coloca los trozos de bacalao en la sartén, con la piel hacia abajo. Vuelve a poner la sartén en el fuego a fuego bajo y cocina lentamente durante unos 10-15 minutos, moviendo la sartén suavemente de vez en cuando para que el bacalao suelte su gelatina y la salsa emulsione.\n",
            "\n",
            "4. Cuando la salsa haya espesado y tenga una textura cremosa, retira la sartén del fuego. Ajusta la sazón con sal y pimienta al gusto.\n",
            "\n",
            "5. Sirve el Bacalao al Pil Pil caliente, espolvoreado con perejil fresco picado por encima.\n",
            "\n",
            "Consejos adicionales:\n",
            "- Asegúrate de desalar correctamente el bacalao antes de utilizarlo en la receta. Puedes hacerlo remojándolo en agua fría durante al menos 24 horas, cambiando el agua varias veces.\n",
            "- La clave para que el Bacalao al Pil Pil quede perfecto es cocinarlo a fuego bajo y mover la sartén suavemente para que la salsa emulsione correctamente.\n",
            "- Acompaña este plato con unas rebanadas de pan sin gluten para disfrutar de la deliciosa salsa.\n",
            "- Puedes servir el Bacalao al Pil Pil con unas patatas cocidas o arroz blanco como guarnición.\n",
            "\n",
            "¡Disfruta de este plato tradicional español sin gluten con un toque gourmet en la comodidad de tu hogar!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puede observarse como funciona correctamente lo implementado ya que he cambiado los parámetros y la receta que ha generado tiene sentido puesto que esta teniendo en cuenta los inputs del usuario. Por ello, doy por concluido este primer apartado."
      ],
      "metadata": {
        "id": "TdFTdXNRzyik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ejercicio 2: Sistema de Resúmenes y Puntos Clave con LangChain o OpenAI"
      ],
      "metadata": {
        "id": "hDjTaq-H0H1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El enunciado de este segundo ejercicio es el siguiente:  En este ejercicio, deberás implementar un sistema que, dado un texto, genere un resumen y liste los puntos clave utilizando LangChain o OpenAI. Este ejercicio tiene como objetivo practicar y asimilar la programación vista durante las clases.\n",
        "\n",
        "En este segundo apartado se solicita que en base a un texto, el modelo sea capaz de generar un resumen y de extraer los puntos más importantes del texto, Para ello, es posible utilizar tanto OpenAI como LangChain. En mi caso, voy a decantarme por utilizar LangChain para generar el resumen porque me parece que ofrece más posibilidades que OpenAI ya que permite acceder al repositorio de HuggingFace donde existen infinidad de modelos y también me permite implementar una metodología distinta a la implementada en el primer apartado.\n",
        "\n",
        "Por otro lado, por también trabajar la otra metodología, los puntos claves del resumen los voy a generar con OpenAI de forma similar a como lo he hecho en el ejercicio 1.\n",
        "\n",
        "El proceso que voy a seguir para resolver este ejercicio va a ser el siguiente:\n",
        "\n",
        "* En primer lugar, voy a generar una función que se encargue de importar el modelo de HuggingFace.\n",
        "* En segundo lugar, voy a generar el resumen del texto en base al modelo especificado en la primera función.\n",
        "* Por último, generaré los puntos claves del resumen por medio de OpenAI.\n",
        "\n",
        "Creo esta metodología puede ser muy interesante ya que si se implementa correctamente va a permitir automatizar todo el proceso de forma sencilla. Es por eso que definido el proceso, instalo en primer lugar las librerías necesarias.\n"
      ],
      "metadata": {
        "id": "XWteiO5h0TyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langchain-openai langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvThXPWAzckm",
        "outputId": "9ad968e1-a2f7-42f8-f5db-41fbe9c45dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.0-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai\n",
            "  Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_core-0.2.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.62-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.3/122.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.30.2)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2023.12.25)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.2.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (0.14.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, tiktoken, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain_community\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.6 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.0 langchain-core-0.2.1 langchain-openai-0.1.7 langchain-text-splitters-0.2.0 langchain_community-0.2.0 langsmith-0.1.62 marshmallow-3.21.2 mypy-extensions-1.0.0 orjson-3.10.3 packaging-23.2 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar genero una función para cargar el modelo de HuggingFaceHub. Esto creo que puede ser interesante para que se pueda probar a generar resúmenes con otros modelos. Simplemente cambiando el ID del modelo, la función devuelve el modelo. Es importante resaltar que el modelo que se desee probar debe haber sido entrenado para resumir textos."
      ],
      "metadata": {
        "id": "5gZHGdhS62Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "def cargar_modelo(repo_id):\n",
        "    try:\n",
        "        modelo = AutoModelForSeq2SeqLM.from_pretrained(repo_id)\n",
        "        tokenizador = AutoTokenizer.from_pretrained(repo_id)\n",
        "        return modelo, tokenizador\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el modelo: {e}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "BQOMnNvM6SOK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seguidamente voy a implementar una función para generar el resumen. Al igual que antes, creo que es interesante implementarlo con una función porque así de este modo se pueden generar resumenes de forma más sencilla y automática. Para ello, en el primer paso de la función se carga el modelo y su tokenizador asociado utilizando la función definida en el apartado anterior. Cargado el tokenizador, se procesa el texto y una vez procesado, se utiliza el modelo para generar el resumen."
      ],
      "metadata": {
        "id": "sDin96939z4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_resumen(texto, repo_id):\n",
        "    # Cargar el modelo y el tokenizador\n",
        "    modelo, tokenizador = cargar_modelo(repo_id)\n",
        "    if modelo is None or tokenizador is None:\n",
        "        print(\"No se pudo cargar el modelo.\")\n",
        "        return None\n",
        "\n",
        "    # Preprocesar el texto\n",
        "    texto_preprocesado = tokenizador(texto, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
        "\n",
        "    # Generar el resumen\n",
        "    resumen_ids = modelo.generate(input_ids=texto_preprocesado[\"input_ids\"],\n",
        "                                  attention_mask=texto_preprocesado[\"attention_mask\"],\n",
        "                                  max_length=1024, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decodificar el resumen\n",
        "    resumen_texto = tokenizador.decode(resumen_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return resumen_texto\n"
      ],
      "metadata": {
        "id": "PSlYZTme9aUw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez que se genera el texto, el enunciado solicita que se genere los puntos clave del mismo. El resumen lo voy a generar utilizando modelos de HuggingFace pero para generar los puntos claves lo voy a hacer por medio de OpenAI como en el apartado anterior. La función va a ser parecida a la del ejercicio 1 pero teniendo en cuenta que ahora la entrada como prompt es el resumen generado por la función anterior."
      ],
      "metadata": {
        "id": "u1c7RPgzAgq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def obtener_puntos_clave(resumen, topico):\n",
        "\n",
        "    # Inicializar el cliente de OpenAI con tu API key\n",
        "    client = OpenAI (api_key='sk-proj-jRwv8xxafg5nXJ00E13YT3BlbkFJtnk6RJhN4dpoh3sikN6H')\n",
        "\n",
        "    prompt = (f\"Based on the summary, extract the key points of it. The summary is: {resumen}\")\n",
        "\n",
        "    try:\n",
        "        # Generar puntos clave utilizando la API de OpenAI\n",
        "        response = client.chat.completions.create(\n",
        "            model='gpt-3.5-turbo',\n",
        "            messages = [\n",
        "              {\"role\": \"system\", \"content\": f\"You are an expert in {topico}\"},\n",
        "              {\"role\": \"user\", \"content\": prompt}\n",
        "              ],\n",
        "    temperature = 0.5)\n",
        "\n",
        "        # Devolver los puntos clave generados por la API\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al obtener puntos clave: {e}\")\n",
        "        return None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WMuIbWr9_VUg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementadas las funciones, procedo a realizar una prueba. Como puede apreciarse en la función anterior, he especificado a OpenAI que se comporte como si fuera un experto en función del texto que se introduzca porque a la hora de resumir textos es importante tener en cuenta el tópico del mismo ya que en base a ello se debe utilizar un modelo u otro. Esto es así porque los modelos se entrenan con datos y se ajustan en base a esos datos. Si se utiliza un tipo de dato con el que modelo no ha sido entrenado, lo normal es que no ofrezca el mismo rendimiento que si se utilizará con un tipo de dato con el que ha sido entrenado. Es decir, por ejemplo, si un modelo ha sido entrenado para generar texto en inglés sobre el tópico de viajes, si se le introduce un texto en español que trata de geografía, lo normal será que su rendimiento no sea tan bueno. Es por eso por lo que es importante conocer el propósito del modelo a utilizar y sus datos de entrenamiento."
      ],
      "metadata": {
        "id": "j-ix-dLtC0Cy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la primera prueba, voy a intentar resumir noticias escritas en inglés ya que considero que en las noticias las ideas clave suelen ser una o dos y lo demás puede ser omitido. Es por eso que necesito encontrar un modelo que haya sido entrenado con noticias en inglés. Investigando la página de HuggingFace es posible apreciar como existen varios modelos que podrían adaptarse pero tras realizar una comparación voy a decanterme por el modelo t5-small. Este modelo ha sido desarrollado por google por lo que ha sido ampliamente utilizado y probado y además de su buen desempeño en textos cortos y medianos, destaca por su eficiencia computacional. Es por eso que en la primera prueba voy a utilizar este modelo.\n",
        "\n",
        "El texto que he elegido para la prueba lo he extraido del periódico* The Times* y he intentado elegir una noticia neutra. Para seguir teniendo el código organizado, he creado una función para que el usuario ingrese el texto que desea resumir."
      ],
      "metadata": {
        "id": "-q1y-bAJBRBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solicitar_texto():\n",
        "    # Solicitar al usuario que ingrese el texto\n",
        "    texto = input(\"Por favor, ingrese el texto que desea resumir: \")\n",
        "\n",
        "    return texto"
      ],
      "metadata": {
        "id": "3fxgi4vTDRfh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ingresar el texto\n",
        "texto = solicitar_texto()\n",
        "\n",
        "# Especificar el ID del modelo\n",
        "repo_id = \"t5-small\"\n",
        "\n",
        "# Llamar a la función para generar el resumen\n",
        "resumen = generar_resumen(texto, repo_id)\n",
        "\n",
        "\n",
        "# Tópico: Es importante especificarlo para que se le pueda indicar a OpenAI como debe comportarse\n",
        "topico = \"politics\"\n",
        "\n",
        "# Llamo a la función para obtener los puntos clave\n",
        "puntos_clave = obtener_puntos_clave(resumen, topico)\n",
        "\n",
        "# Imprimo los puntos clave por pantalla\n",
        "print(\"------------------------------------------\")\n",
        "print(\"Texto a resumir:\")\n",
        "print(texto)\n",
        "print()\n",
        "\n",
        "# Imprimir el resumen por pantalla\n",
        "print(\"Resumen del texto:\")\n",
        "print(resumen)\n",
        "print()\n",
        "\n",
        "print(\"Puntos clave del resumen:\")\n",
        "print(puntos_clave)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StGRDvL5Dnjn",
        "outputId": "3e150e8b-b721-4ede-fec4-8b7f7de829fd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Por favor, ingrese el texto que desea resumir: Donald Trump has promised that his relationship with President Putin will secure the release of Evan Gershkovich, the Wall Street Journal reporter being held in Moscow on espionage charges, if he wins the American presidential election in November.  Gershkovich, 32, was arrested in Russia’s Urals region in March last year on suspicion of seeking to obtain defence secrets for United States intelligence. He faces up to 20 years in a penal camp if convicted. Gershkovich and the WSJ have denied the allegation, while the White House has classified him as “wrongfully detained”.  “Evan Gershkovich, the Reporter from The Wall Street Journal, who is being held by Russia, will be released almost immediately after the Election, but definitely before I assume Office. He will be HOME, SAFE, AND WITH HIS FAMILY,” Trump wrote on his Truth Social website.  Donald Trump made the claims on Truth Social but the Kremlin says President Putin has had no contact with the former president Donald Trump made the claims on Truth Social but the Kremlin says President Putin has had no contact with the former president  “Vladimir Putin, President of Russia, will do that for me, but not for anyone else, and WE WILL BE PAYING NOTHING!” he wrote.\n",
            "------------------------------------------\n",
            "Texto a resumir:\n",
            "Donald Trump has promised that his relationship with President Putin will secure the release of Evan Gershkovich, the Wall Street Journal reporter being held in Moscow on espionage charges, if he wins the American presidential election in November.  Gershkovich, 32, was arrested in Russia’s Urals region in March last year on suspicion of seeking to obtain defence secrets for United States intelligence. He faces up to 20 years in a penal camp if convicted. Gershkovich and the WSJ have denied the allegation, while the White House has classified him as “wrongfully detained”.  “Evan Gershkovich, the Reporter from The Wall Street Journal, who is being held by Russia, will be released almost immediately after the Election, but definitely before I assume Office. He will be HOME, SAFE, AND WITH HIS FAMILY,” Trump wrote on his Truth Social website.  Donald Trump made the claims on Truth Social but the Kremlin says President Putin has had no contact with the former president Donald Trump made the claims on Truth Social but the Kremlin says President Putin has had no contact with the former president  “Vladimir Putin, President of Russia, will do that for me, but not for anyone else, and WE WILL BE PAYING NOTHING!” he wrote.\n",
            "\n",
            "Resumen del texto:\n",
            "has promised that his relationship with President Putin will secure the release of Evan Gershkovich. Gershkovich, the Wall Street Journal reporter being held in Moscow on espionage charges, will be released if he wins the election in November. the Kremlin says President Putin has had no contact with the former president.\n",
            "\n",
            "Puntos clave del resumen:\n",
            "Key points from the summary:\n",
            "1. The former president has promised that his relationship with President Putin will secure the release of Evan Gershkovich.\n",
            "2. Gershkovich, a Wall Street Journal reporter, is being held in Moscow on espionage charges.\n",
            "3. Gershkovich will be released if the former president wins the election in November.\n",
            "4. The Kremlin denies that President Putin has had any contact with the former president.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como puede verse tanto el resumen como los puntos claves funcionan perfectamente ya que ambos dos cumplen con su cometido. El resumen ofrece brevemente las ideas principales de la noticia y luego gracias a OpenAI se genera un listado con los puntos claves. Esto sin duda es muy interesante porque especificando unicamente el modelo, el topico de la noticia e indicando la propia noticia, se puede extraer su información más relevante.\n",
        "\n",
        "Por probar otro modelo, voy a realizar el mismo ejercicio pero en este caso resumiendo una oferta de empleo en inglés. En este caso he investigado HuggingFace y voy a utilizar el modelo facebook/bart-base porque la arquitectura de los modelos BART es perfecta para generar textos y porque posee una gran capacidad para capturar patrones complejos."
      ],
      "metadata": {
        "id": "dlonpA9p7nJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ingresar el texto\n",
        "texto = solicitar_texto()\n",
        "\n",
        "# Especificar el ID del modelo\n",
        "repo_id = \"facebook/bart-large-cnn\"\n",
        "\n",
        "# Llamar a la función para generar el resumen\n",
        "resumen = generar_resumen(texto, repo_id)\n",
        "\n",
        "# Tópico: Es importante especificarlo para que se le pueda indicar a OpenAI como debe comportarse\n",
        "topico = \"hiring\"\n",
        "\n",
        "# Llamo a la función para obtener los puntos clave\n",
        "puntos_clave = obtener_puntos_clave(resumen, topico)\n",
        "\n",
        "# Imprimo los puntos clave por pantalla\n",
        "print(\"------------------------------------------\")\n",
        "print(\"Texto a resumir:\")\n",
        "print(texto)\n",
        "print()\n",
        "\n",
        "# Imprimir el resumen por pantalla\n",
        "print(\"Resumen del texto:\")\n",
        "print(resumen)\n",
        "print()\n",
        "\n",
        "print(\"Puntos clave del resumen:\")\n",
        "print(puntos_clave)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0aBnhDLEu4b",
        "outputId": "8a868fba-04ea-4fb1-8430-191997376753"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Por favor, ingrese el texto que desea resumir: We are hiring a Junior AI Engineer to support the engineering team with multiple aspects of agent development, including system design, prompt engineering, and integrations.   You will play a crucial role in designing and implementing agentic workflows, developing & refining AI prompts, integrating agents with various data sources and software tools, and evaluating model/agent performance. This role offers a unique opportunity to gain hands-on experience in developing AI systems for real-world applications. The ideal candidate will have some experience with scripting languages (Python & TypeScript required), a deep personal interest in AI, and a strong sense of system design.   Responsibilities  Agentic Workflow Design: Collaborate with the AI team to conceptualize and build structured workflows for AI agents, defining clear objectives and tasks. Prompt Engineering: Craft effective prompts to guide agent behaviors and interactions, optimizing for clarity, accuracy, speed and desired outcomes. Use technology to accelerate and improve prompt testing & optimization. Integrations: Connect AI agents to databases, APIs, and traditional software tools, enabling seamless data exchange and automation. Testing & Evaluation: Develop and execute test plans to assess agent performance, identifying areas for improvement and iterating on prompts and workflows. Documentation: Maintain detailed documentation of agent designs, workflows, prompts, and testing methodologies to ensure reproducibility and knowledge sharing. Learning & Development: Stay abreast of the latest advancements in AI agent technologies, exploring new techniques and tools to enhance our solutions. Share your learnings with the team to improve our approach to AI development.   Qualifications  1-3 years of experience in software development (AI/ML experience a plus) Proficiency in Python or TypeScript for scripting & integrations Basic understanding of AI agents, natural language processing, and machine learning concepts Experience using LLMs and LLM-enabled programs in a professional, academic, or personal context, including going “behind the curtain” to learn more about how they work and how to use them effectively Ability to conduct analysis of model performance via mathematical models and data visualization Strong technical and creative writing skills Bonus qualifications  Experience with LangChain or other AI agent frameworks Familiarity with prompt engineering best practices Familiarity with data structures for ML and AI programs, including RAG and vector databases Experience working with cloud technology stack (e.g., AWS or GCP) and developing machine learning models in a cloud environment\n",
            "------------------------------------------\n",
            "Texto a resumir:\n",
            "We are hiring a Junior AI Engineer to support the engineering team with multiple aspects of agent development, including system design, prompt engineering, and integrations.   You will play a crucial role in designing and implementing agentic workflows, developing & refining AI prompts, integrating agents with various data sources and software tools, and evaluating model/agent performance. This role offers a unique opportunity to gain hands-on experience in developing AI systems for real-world applications. The ideal candidate will have some experience with scripting languages (Python & TypeScript required), a deep personal interest in AI, and a strong sense of system design.   Responsibilities  Agentic Workflow Design: Collaborate with the AI team to conceptualize and build structured workflows for AI agents, defining clear objectives and tasks. Prompt Engineering: Craft effective prompts to guide agent behaviors and interactions, optimizing for clarity, accuracy, speed and desired outcomes. Use technology to accelerate and improve prompt testing & optimization. Integrations: Connect AI agents to databases, APIs, and traditional software tools, enabling seamless data exchange and automation. Testing & Evaluation: Develop and execute test plans to assess agent performance, identifying areas for improvement and iterating on prompts and workflows. Documentation: Maintain detailed documentation of agent designs, workflows, prompts, and testing methodologies to ensure reproducibility and knowledge sharing. Learning & Development: Stay abreast of the latest advancements in AI agent technologies, exploring new techniques and tools to enhance our solutions. Share your learnings with the team to improve our approach to AI development.   Qualifications  1-3 years of experience in software development (AI/ML experience a plus) Proficiency in Python or TypeScript for scripting & integrations Basic understanding of AI agents, natural language processing, and machine learning concepts Experience using LLMs and LLM-enabled programs in a professional, academic, or personal context, including going “behind the curtain” to learn more about how they work and how to use them effectively Ability to conduct analysis of model performance via mathematical models and data visualization Strong technical and creative writing skills Bonus qualifications  Experience with LangChain or other AI agent frameworks Familiarity with prompt engineering best practices Familiarity with data structures for ML and AI programs, including RAG and vector databases Experience working with cloud technology stack (e.g., AWS or GCP) and developing machine learning models in a cloud environment\n",
            "\n",
            "Resumen del texto:\n",
            "We are hiring a Junior AI Engineer to support the engineering team with multiple aspects of agent development. You will play a crucial role in designing and implementing agentic workflows, developing & refining AI prompts, integrating agents with various data sources and software tools. This role offers a unique opportunity to gain hands-on experience in developing AI systems for real-world applications.\n",
            "\n",
            "Puntos clave del resumen:\n",
            "Key Points:\n",
            "- Hiring a Junior AI Engineer\n",
            "- Support engineering team in agent development\n",
            "- Design and implement agentic workflows\n",
            "- Develop and refine AI prompts\n",
            "- Integrate agents with data sources and software tools\n",
            "- Gain hands-on experience in developing AI systems for real-world applications\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso también es posible apreciar como con este nuevo modelo es posible generar un resumen y los puntos claves del mismo. De lo que parecía mucha información sobre lo que solicitaba la empresa, se ha obtenido un listado y un resumen que recoge la esencia la oferta de empleo. Esto es muy intersante también porque se podría implementar algún automatismo que procesará ofertas de empleo de portales de empleo y para cada una, generará el resumen y los puntos más importantes. Esto por ejemplo sería interesante para empresas que se dedican a la contratación ya que podría tener una visión global de lo que se busca en el mercado.\n",
        "\n",
        "Por último, destacar que en este caso los dos ejemplos que se han elegido han sido basados en lo que en mi opinión personal era interesante resumir. Esto no quiere decir que la solución no pueda ser utilizada con otros textos. Asimismo, las dos soluciones que se han generado han sido en inglés porque globalmente el idioma oficial de programación y por lo tanto de HuggingFace es en inglés y como consecuencia hay muchos más recursos en este idioma. Es por eso que las dos soluciones son en este idioms. No obstante, también se podría utilizar la solución con modelos en español, ya que lo único que habría que cambiar sería el ID del modelo. Esto es la ventaja de haberlo implementado con funciones."
      ],
      "metadata": {
        "id": "nSfwat1u_4Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agradecimientos"
      ],
      "metadata": {
        "id": "JosfnQ2PBJQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para finalizar esta práctica deseo agradecer a nuestro profesor Marc Mayol por todas las horas que ha invertido en preparar las clases, impartirlas, transmitir todos los conocimientos, resolver todas las dudas y estar pendiente de que entendamos los conceptos claves del prompt engineering. Aunque este módulo haya sido corto creo que nos ha permitido conocer los conceptos básicos del prompt engineering y forma en que se puede automatizar por medio de scripts. Desde mi punto de vista, esto es muy interesante dado que puede permitir generar soluciones que pueden ser muy valiosas a la hora de tomar decisiones Además, en mi opinión creo que fue un acierto que Marc dedicará mucho tiempo a explicar HuggingFace ya que el hecho de conocer este repositorio permite generar mejores soluciones gracias a los modelos ya preentrenados. Es por eso por lo que quiero expresar mi más sincero agradecimiento a nuestro profesor.\n",
        "\n",
        "Por último, agradezco también a la academia Keepcoding por haber introducido este módulo en el bootcamp, el cual creo que va a ser muy beneficioso para mi futuro."
      ],
      "metadata": {
        "id": "p_rQlnaNrvIc"
      }
    }
  ]
}